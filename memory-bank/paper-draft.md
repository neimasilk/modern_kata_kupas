# **Revisiting Rule-Based Indonesian Sub-word Separation for Enhanced LLM Performance and Low-Resource NLP**

## **Chapter 1: Introduction**

### **1.1 Background and Problem Statement**

Indonesian, as an agglutinative language, exhibits a rich and complex morphological system. Words are frequently formed by affixing one or more morphemes (prefixes, suffixes, infixes, confixes) to a root word, or through reduplication. This productive word formation process, while systematic, leads to a significant challenge in Natural Language Processing (NLP): vocabulary explosion. A single root word can spawn numerous inflected or derived forms, each potentially treated as a unique token by standard NLP models. This issue was highlighted in previous research (e.g., Amien et al., 2022 \- *referencing the foundational concepts of the original 2020 paper*), which demonstrated that rule-based sub-word separation could effectively reduce vocabulary size and improve performance in Neural Machine Translation (NMT) by mitigating the rare word problem.

In the current era (circa 2025 and beyond), the NLP landscape is dominated by Large Language Models (LLMs) that primarily rely on data-driven sub-word tokenization methods such as Byte Pair Encoding (BPE) (Sennrich et al., 2016), WordPiece (Wu et al., 2016), and SentencePiece (Kudo & Richardson, 2018). While these methods have proven highly effective for many languages, morphologically rich languages like Indonesian continue to present unique challenges:

1. **Semantic Opacity of Sub-word Units:** Standard statistical sub-word tokenizers often produce segments that do not align with linguistic morpheme boundaries. For example, a word like "mempertanggungjawabkannya" might be split into `memper`, `tanggun`, `gjawab`, `kannya` by BPE, where `tanggun` and `gjawab` are not meaningful morphemes on their own. This can obscure the compositional meaning of the word from the model, potentially hindering deeper semantic understanding and generalization.  
2. **Challenges in Low-Resource Scenarios:** Data-driven sub-word methods inherently require vast amounts of text data to learn effective segmentation patterns. In low-resource Indonesian NLP tasks, or when fine-tuning LLMs for highly specific domains with limited data, these methods may underperform or produce suboptimal segmentations. Rule-based morphological segmentation, which relies on linguistic knowledge rather than data frequency, could offer a more robust alternative or a valuable complement in such scenarios.  
3. **Interpretability and Controllability:** The sub-word units generated by statistical methods are often difficult to interpret from a linguistic standpoint. Morphologically-aware segmentation, on the other hand, provides segments (root words and affixes) that are inherently interpretable. This can be advantageous for error analysis, model debugging, and applications requiring more controlled or explainable text generation.  
4. **Efficiency for LLM Fine-tuning and Inference:** The large vocabularies generated by standard tokenizers for agglutinative languages can lead to larger model embedding layers and potentially less efficient fine-tuning and inference. A pre-processing step that systematically reduces words to their morphological components could lead to a more compact and meaningful vocabulary, potentially improving computational efficiency and parameter sharing for LLMs.

This research revisits and aims to enhance the concept of rule-based Indonesian sub-word separation, adapting it for the contemporary NLP ecosystem dominated by LLMs.

### **1.2 Proposed Solution: "ModernKataKupas"**

We propose "ModernKataKupas," an enhanced rule-based algorithm for Indonesian sub-word separation. The core principle, inspired by the "Amien Separator" concept (Amien et al., 2022), is to decompose Indonesian words into a canonical sequence of `prefix(es) ~ root_word ~ suffix(es) ~ particle(s) ~ possessive_pronoun(s)`, where `~` denotes a morpheme boundary, and special markers denote processes like reduplication. This process is designed to be meaning-preserving and fully reconstructible, allowing the original word form to be recovered from its segmented components.

The key aspects of ModernKataKupas include:

* Leveraging a robust Indonesian stemmer to identify the root word.  
* Employing string alignment techniques to pinpoint differences between the original word and its root.  
* Applying a refined and expanded set of morphological rules to accurately parse these differences into valid affixes, handling complex phenomena like derivational layering, various types of reduplication, and systematic morphophonemic changes.  
* Focusing on its application as a pre-processing step for Indonesian text or as an auxiliary feature to augment standard tokenization methods for LLMs and other NLP models.

### **1.3 Research Questions**

This study aims to answer the following research questions:

1. **RQ1:** To what extent can the ModernKataKupas algorithm reduce vocabulary size and out-of-vocabulary (OOV) rates for diverse, contemporary Indonesian text corpora compared to word-level and standard data-driven sub-word tokenization methods?  
2. **RQ2:** Does applying ModernKataKupas segmentation as a pre-processing step improve the performance of pre-trained Large Language Models (LLMs) on various downstream Indonesian NLP tasks (e.g., text classification, sentiment analysis, question answering, named entity recognition) when compared to or combined with standard sub-word tokenization techniques?  
3. **RQ3:** How does the performance of ModernKataKupas compare to purely data-driven sub-word tokenization (e.g., BPE, SentencePiece) in terms of both quantitative metrics on downstream tasks and qualitative aspects like segment interpretability and consistency for Indonesian, particularly for complex morphological forms?  
4. **RQ4:** Can ModernKataKupas segmentation enhance the performance of Transformer-based Neural Machine Translation (NMT) systems for Indonesian-English language pairs, particularly in low-resource or domain-specific translation scenarios, by providing more explicit morphological cues?

### **1.4 Anticipated Contributions**

This research is expected to make the following contributions:

1. **An Enhanced Algorithm and Open-Source Tool:** Development and public release of the "ModernKataKupas" algorithm, an updated and more comprehensive rule-based Indonesian sub-word separator, with improved handling of complex morphology.  
2. **Empirical Evaluation on Modern Data:** Quantitative analysis of the algorithm's impact on vocabulary characteristics (size, OOV rates, morpheme distribution) using large, contemporary Indonesian datasets.  
3. **Comparative NLP Task Performance Analysis:** Rigorous empirical evaluation of ModernKataKupas's effectiveness (as a standalone preprocessor or hybrid approach) against standard sub-word tokenizers for fine-tuning pre-trained LLMs on a suite of Indonesian downstream NLP tasks.  
4. **NMT Performance in Constrained Settings:** Assessment of its utility in improving Indonesian NMT, especially for Transformer models in low-resource or specialized domains.  
5. **Insights for Agglutinative Languages:** Providing new insights into the role and benefits of explicit morphological awareness in the age of LLMs, particularly for handling the complexities of agglutinative languages like Indonesian.

### **1.5 Paper Structure**

This paper is organized as follows: Chapter 2 reviews related work in Indonesian morphology, sub-word tokenization, and the integration of morphology in neural models. Chapter 3 details the proposed "ModernKataKupas" algorithm, including its core components, rule architecture, enhancements for handling complex morphological phenomena, and specific guidelines for implementation. Chapter 4 describes the experimental setup, including datasets, models, and evaluation metrics. Chapter 5 presents and discusses the results of our experiments. Finally, Chapter 6 concludes the paper and outlines directions for future work.

## **Chapter 2: Literature Review**

This chapter reviews existing literature relevant to Indonesian morphology, sub-word tokenization techniques prevalent in modern NLP, the incorporation of morphological information into neural models, and challenges in low-resource NLP scenarios.

### **2.1 Indonesian Morphology and Stemming**

Indonesian is characterized by its rich agglutinative morphology, where words are commonly formed by adding prefixes, suffixes, infixes, and confixes to a root word, or through reduplication (Sneddon, 1996; Alwi et al., 2003). This complexity has spurred significant research in Indonesian stemming, the process of reducing inflected or derived words to their root form.

Notable Indonesian stemmers include:

* **Nazief and Adriani (1996):** One of the earliest rule-based stemmers for Indonesian, employing affix stripping rules.  
* **Asian (2007):** Proposed an enhanced confix-stripping approach, building upon Nazief and Adriani's work, and introduced more comprehensive morphological rules.  
* **Sastrawi (Swasono et al., 2016; Alfina et al., 2017):** A popular open-source Indonesian stemmer based on the Enhanced Confix Stripping (ECS) algorithm. It uses a dictionary of root words and a set of affix stripping rules, forming a strong baseline for rule-based morphological analysis in Indonesian.  
* **Amien et al. (2022):** The foundational work for this research, which proposed a rule-based sub-word *separator* (distinct from a pure stemmer) to transform words into root and affix components for NMT. This work demonstrated vocabulary reduction and BLEU score improvements, highlighting the potential of retaining affix information. The "Amien Separator" (referred to as "Amien Stemmer" in the original publication due to its stemming-like process for component identification) used string alignment and a set of rules to achieve this separation.

The primary goal of traditional stemmers is to map various word forms to a single root for tasks like information retrieval. However, for tasks requiring nuanced semantic understanding, such as machine translation or text generation with LLMs, the information carried by affixes (e.g., tense, aspect, voice, derivational meaning) is crucial and should ideally be preserved in a structured manner. The "ModernKataKupas" approach builds on the idea of separation rather than mere stripping.

### **2.2 Sub-word Tokenization in Natural Language Processing**

To handle the OOV problem and manage vocabulary size, modern NLP models, especially LLMs, heavily rely on sub-word tokenization algorithms. These algorithms break words into smaller, frequently occurring units.

* **Byte Pair Encoding (BPE) (Sennrich et al., 2016):** Starts with a vocabulary of individual characters and iteratively merges the most frequent pair of adjacent units to form new sub-word units until a desired vocabulary size is reached. Used in models like GPT.  
* **WordPiece (Wu et al., 2016; Schuster & Nakajima, 2012):** Similar to BPE but uses a likelihood-based merging criterion. Employed by BERT and its variants.  
* **SentencePiece (Kudo & Richardson, 2018):** A language-independent sub-word tokenizer and detokenizer that treats sentences as sequences of Unicode characters. It can train BPE or Unigram models directly from raw text.  
* **Unigram Language Model (Kudo, 2018):** Starts with a large set of candidate sub-words and iteratively removes units that least affect the overall corpus likelihood, optimizing for a probabilistic language model over sub-words.

While these methods are data-driven and language-agnostic, their purely statistical nature means the resulting sub-words often lack direct linguistic interpretability, especially for morphologically complex languages. Segments may cross morpheme boundaries or consist of partial morphemes, potentially making it harder for models to learn systematic morphological patterns.

### **2.3 Morphology in Neural Models**

The importance of morphology for NLP has led to various attempts to incorporate it into neural architectures:

* **Character-Level Models (Kim et al., 2016; Ling et al., 2015):** These models learn representations directly from characters, implicitly capturing morphological information. However, they can be computationally intensive and may struggle with long-range dependencies.  
* **Hybrid Models:** Combine word-level embeddings with character-level or sub-word representations to leverage both lexical and morphological cues (e.g., FastText (Bojanowski et al., 2017)).  
* **Explicit Morphological Features:** Some approaches involve providing models with explicit morphological tags or features (e.g., part-of-speech tags that include morphological information).  
* **Morphologically-Aware Segmentation:** Research has explored using linguistic knowledge to guide sub-word segmentation (e.g., Morfessor (Creutz & Lagus, 2007)), aiming for segments that correspond more closely to morphemes. Recent work also explores learning morphological segmentations (Kann et al., 2018\) or integrating morphological analyzers into the tokenization pipeline for LLMs (Rust et al., 2021).

For agglutinative languages, effectively modeling morphology remains an active area of research. LLMs, despite their scale, can still benefit from representations that make morphological structure more explicit, potentially improving sample efficiency, generalization to unseen inflected forms, and performance on morphologically sensitive tasks.

### **2.4 Low-Resource NLP and Linguistic Knowledge**

In low-resource settings, where large training corpora are unavailable, data-driven methods (including statistical sub-word tokenizers and large-scale model pre-training) face significant limitations. In such scenarios, leveraging linguistic knowledge becomes crucial.

Rule-based systems, informed by the grammatical structure of a language, can provide a strong baseline or a valuable component when data is scarce. For Indonesian, its relatively systematic morphology makes a rule-based approach to sub-word segmentation particularly appealing for low-resource NLP tasks, including domain-specific NMT or LLM fine-tuning where in-domain data is limited. The original work by Amien et al. (2022) provided initial evidence for this in NMT. This paper seeks to extend this investigation to the current LLM paradigm and a broader range of NLP tasks.

### **2.5 Positioning the Current Research**

This research builds upon the premise that explicit, rule-based morphological segmentation can offer advantages for Indonesian NLP, even in the age of powerful LLMs and sophisticated data-driven tokenizers. It aims to:

1. Provide an updated and more robust rule-based separator ("ModernKataKupas") with more comprehensive handling of Indonesian morphology than previously explored.  
2. Systematically evaluate its impact not just on NMT but also on downstream tasks powered by LLMs, focusing on contemporary datasets and models.  
3. Conduct a direct comparison with, and explore synergies with, standard sub-word tokenization methods, addressing a gap in the original paper and current literature for Indonesian.  
4. Offer a linguistically-informed alternative or complement for handling Indonesian text, particularly beneficial in low-resource or specialized contexts where data-driven methods may falter or where interpretability and controllability are paramount.

The goal is not necessarily to replace statistical sub-word tokenizers entirely, but to investigate whether a linguistically-grounded, rule-based approach can enhance their utility or offer a superior alternative under specific conditions for Indonesian.

## **Chapter 3: Proposed Methodology: "ModernKataKupas" Algorithm**

This chapter details the "ModernKataKupas" algorithm, a rule-based Indonesian sub-word separator. The algorithm is designed to decompose Indonesian words into their constituent morphemes—root word(s), prefixes, suffixes, and markers for processes like reduplication—in a way that is both linguistically informed and computationally implementable. The output format aims for a sequence like `prefix1~prefix2~ROOT~suffix1~suffix2~PARTICLE~POSSESSIVE_PRONOUN` or `ROOT~REDUPLICATION_MARKER~AFFIXES`, where `~` acts as a morpheme boundary marker.

### **3.1 Overall Algorithmic Architecture**

The ModernKataKupas algorithm processes an input Indonesian word through a sequential pipeline:

Input Word \-\> 1\. Normalization \-\> 2\. Reduplication Handling \-\> 3\. Root Word Identification (Stemming) \-\> 4\. Affix Identification & Separation (using String Alignment & Morphological Rules) \-\> 5\. Output Formatting \-\> Segmented Word A core principle is **reconstructibility**: the segmented output must be convertible back to the original word by applying the inverse morphological rules. This serves as a key validation mechanism.

### **3.2 Foundational Components for Implementation**

1. **Text Normalization Module:**  
     
   * **Input:** Raw word string.  
   * **Process:**  
     * Convert to lowercase.  
     * Define a clear policy for handling punctuation:  
       * Sentence-final punctuation (e.g., `.`, `?`, `!`) should generally be detached and handled as separate tokens by a higher-level sentence tokenizer. This algorithm focuses on word-internal morphology.  
       * Internal hyphens must be preserved as they are crucial for identifying reduplication (e.g., `buku-buku`) and some compounds.  
       * Other special characters or non-standard symbols should be removed or normalized based on a predefined list.  
   * **Output:** Normalized word string.

   

2. **Root Word Dictionary (`kamus_dasar`):**  
     
   * **Structure:** A Python `set` for efficient `O(1)` average time complexity lookups.  
   * **Content:** Based on PySastrawi's `kata-dasar.txt`, augmented with:  
     * Entries from the latest Kamus Besar Bahasa Indonesia (KBBI V). This requires a script to parse and integrate KBBI data.  
     * Validated root words from contemporary Indonesian corpora (e.g., OSCAR, cleaned Common Crawl). This involves frequency analysis and potentially manual review or heuristics to filter noise.  
     * A list of common unsegmentable loanwords (e.g., "komputer", "internet") and frozen compounds (e.g., "olahraga", "kacamata", "tanggungjawab" if treated as a single lexical unit by the stemmer) to prevent incorrect segmentation.  
   * **Maintenance:** A clear process for updating this dictionary will be crucial.

   

3. **Underlying Stemmer (`stemmer_internal`):**  
     
   * **Choice:** PySastrawi's `StemmerFactory().create_stemmer()` will be the default.  
   * **Interface:** A wrapper function will be created to call `stemmer_internal.stem(word)` and return the root word.  
   * **Consideration:** The stemmer's behavior with already-root words (should return the word itself) and OOV words (should ideally return the word itself or apply minimal, safe transformations) is important.

   

4. **Affix Rule Repository (`aturan_afiks`):**  
     
   * **Structure:** A structured format, preferably JSON or YAML, for maintainability.  
     * Each entry will define an affix, its canonical form, its type (prefix, suffix, particle, possessive), allomorphs, conditions for application/stripping (e.g., preceding/following phoneme, part-of-speech of the root if available), and any morphophonemic changes it induces or undergoes.  
   * **Example (Conceptual JSON for `meN-`):**  
       
     {  
       
       "meN-": {  
       
         "type": "prefix\_derivational",  
       
         "canonical": "meN",  
       
         "allomorphs": \[  
       
           {"surface": "mem", "condition\_root\_starts\_with": \["b","f","v"\], "reconstruct\_root\_initial": null},  
       
           {"surface": "mem", "condition\_root\_starts\_with": \["p"\], "reconstruct\_root\_initial": "p"}, // p luluh  
       
           {"surface": "men", "condition\_root\_starts\_with": \["d","c","j","z"\], "reconstruct\_root\_initial": null},  
       
           {"surface": "men", "condition\_root\_starts\_with": \["t"\], "reconstruct\_root\_initial": "t"}, // t luluh  
       
           {"surface": "meny", "condition\_root\_starts\_with": \["s"\], "reconstruct\_root\_initial": "s"}, // s luluh  
       
           {"surface": "meng", "condition\_root\_starts\_with": \["g","h","q","a","i","u","e","o"\], "reconstruct\_root\_initial": null},  
       
           {"surface": "meng", "condition\_root\_starts\_with": \["k"\], "reconstruct\_root\_initial": "k"}, // k luluh  
       
           {"surface": "me", "condition\_root\_starts\_with": \["l","m","n","r","w","y","ng","ny"\]},  
       
           {"surface": "menge", "condition\_root\_is\_monosyllabic": true, "reconstruct\_root\_initial": null} // e.g., menge-bom  
       
         \]  
       
       },  
       
       "ber-": { /\* ... \*/ },  
       
       "-kan": {  
       
         "type": "suffix\_derivational",  
       
         "canonical": "kan",  
       
         "allomorphs": \[{"surface": "kan"}\]  
       
       }  
       
       // ... other affixes  
       
     }  
       
   * This repository will be loaded at runtime.

### **3.3 Morphological Segmentation Algorithm: "ModernKataKupas" \- Detailed Steps**

#### **3.3.1 Step 1: Normalization**

* Apply the Text Normalization Module (3.2.1) to the input word. Let the result be `normalized_word`.

#### **3.3.2 Step 2: Reduplication Handling**

This step aims to identify and segment reduplicated forms before general affix stripping. The output of this stage will be a `base_form_for_affixation` and a list of `reduplication_segments`.

1. **Full Reduplication (Dwilingga) `X-X`:**  
     
   * **Detection:** Use regex `^([^-]+)-\1$` on `normalized_word`.  
   * **If matched:**  
     * `base_form_for_affixation` \= first part (X).  
     * `reduplication_segments` \= \[`~ulg`\].  
     * Example: `buku-buku` \-\> `base_form_for_affixation` \= "buku", `reduplication_segments` \= \["\~ulg"\].  
     * Example: `mobil-mobilan` \-\> First, this might be treated as `X-Y`. If `Y` (`mobilan`) when processed for affixes yields `mobil~an`, and `X` is `mobil`, then it's `mobil~ulg~an`. A more robust approach is to try stemming both `X` and `Y`. If `stem(X) == stem(Y)`, it's likely dwilingga with potential affixes on the second part or both.  
     * This needs careful ordering: are affixes stripped before or after recognizing the `X-X` pattern for affixed dwilingga? Generally, if the affix applies to the whole reduplicated unit (e.g. `ke-merah-merah-an`), it should be stripped first. If it applies to one part (e.g. `ber-lari-larian`), the reduplication might be processed first. For simplicity, initially assume affixes are stripped from the `base_form_for_affixation`.

   

2. **Full Reduplication with Phonetic Change (Dwilingga Salin Suara) `X-Y`:**  
     
   * **Detection:** Split `normalized_word` by hyphen into `part1` and `part2`.  
     * Check if `part1` is in `kamus_dasar`.  
     * Check if `part2` is a known phonetic variant of `part1` (requires a predefined list of common pairs like `sayur-mayur`, `warna-warni`, `bolak-balik`) OR if `stemmer_internal.stem(part2)` (or variations like `stemmer_internal.stem("meN" + part2)`) relates to `part1`.  
   * **If matched:**  
     * `base_form_for_affixation` \= `part1`.  
     * `reduplication_segments` \= \[`~rs(~` \+ `part2` \+ `)`\]. (e.g., `sayur~rs(~mayur)`).  
     * Example: `sayur-mayur` \-\> `base_form_for_affixation` \= "sayur", `reduplication_segments` \= \["\~rs(\~mayur)"\].

   

3. **Partial Reduplication (Dwipurwa):**  
     
   * **Detection:** This is harder.  
     * Get `root_word_temp = stemmer_internal.stem(normalized_word)`.  
     * If `normalized_word` starts with `root_word_temp`'s first syllable (or C(C)V) repeated, and `normalized_word` ends with `root_word_temp`.  
     * Example: `lelaki`, `root_word_temp` \= `laki`. `lelaki` starts with `le` (approx. `la`) and ends with `laki`.  
   * **If matched:**  
     * `base_form_for_affixation` \= `root_word_temp`.  
     * `reduplication_segments` \= \[`~rp`\].  
     * Example: `lelaki` \-\> `base_form_for_affixation` \= "laki", `reduplication_segments` \= \["\~rp"\].

   

4. **If no reduplication detected:**  
     
   * `base_form_for_affixation` \= `normalized_word`.  
   * `reduplication_segments` \= \[\].

Let `current_word_to_process` be `base_form_for_affixation`.

#### **3.3.3 Step 3: Root Word Identification**

* `root_word = stemmer_internal.stem(current_word_to_process)`.  
* If `root_word == current_word_to_process` (and it's in `kamus_dasar`), the word is likely a base form or a loanword. Proceed to loanword check (3.3.5) or finalize if no affixes are suspected.

#### **3.3.4 Step 4: Core Affix Identification and Separation**

This iterative process uses string alignment and the `aturan_afiks`. Initialize `identified_prefixes = []` and `identified_suffixes = []`.

1. **String Alignment:**  
     
   * Perform Needleman-Wunsch alignment between `current_word_to_process` and `root_word`.  
   * Identify initial differing segment (potential prefixes) and final differing segment (potential suffixes).  
   * Example: `mempermainkan` vs `main` \-\> `prefix_candidate = "memper"`, `suffix_candidate = "kan"`.

   

2. **Iterative Suffix Stripping (from `suffix_candidate`):**  
     
   * Loop (e.g., max 3 times for particles, possessives, derivational):  
     * Attempt to match and strip **Inflectional Particles** (`-lah`, `-kah`, `-tah`, `-pun`) from the right end of `suffix_candidate` or `current_word_to_process`. If match, add to `identified_suffixes` (e.g., `~lah`), update `suffix_candidate`/`current_word_to_process`.  
     * Attempt to match and strip **Possessive Pronouns** (`-ku`, `-mu`, `-nya`). If match, add to `identified_suffixes` (e.g., `~nya`), update.  
     * Attempt to match and strip **Derivational Suffixes** (`-kan`, `-i`, `-an`). If match, add to `identified_suffixes` (e.g., `~kan`), update. Only one of these is typically stripped per cycle unless dealing with complex derivations not handled by the stemmer.  
     * After each strip, check if the new `current_word_to_process` (or its prefix-stripped version) is in `kamus_dasar` or matches `root_word`. If so, suffix stripping might be complete for this layer.  
   * Store suffixes in reverse order of stripping, then reverse list for final output.

   

3. **Iterative Prefix Stripping (from `prefix_candidate`):**  
     
   * Loop (e.g., max 2-3 times for layered prefixes):  
     * Iterate through `aturan_afiks` for prefixes. For each prefix rule:  
       * Check if `prefix_candidate` (or `current_word_to_process`) starts with a surface form of the prefix (e.g., "mem", "per").  
       * Validate the condition (e.g., `root_starts_with`). This requires looking at the `root_word`'s initial character(s) or the character(s) in `current_word_to_process` immediately following the potential prefix.  
       * If a valid prefix is identified:  
         * Add its canonical form (e.g., `meN~`, `per~`) to `identified_prefixes`.  
         * Update `prefix_candidate`/`current_word_to_process` by removing the surface form of the prefix.  
         * **Crucially, if the rule involves elision of a root character (luluh), this must be noted for the reconstruction phase.** The `root_word` obtained from the stemmer already reflects this. The alignment helps confirm the elided character.  
         * Example: `memukul`, `root_word = pukul`. Alignment shows `mem` as prefix part. Rule for `meN-` with `p` matches `mem` and elision of `p`. Prefix is `meN~`. `current_word_to_process` becomes `pukul`.  
     * After each strip, check if `current_word_to_process` matches `root_word`. If so, prefix stripping is complete.

   

4. **Confix Handling (Implicit):**  
     
   * Confixes like `ke-an`, `per-an` are handled by the sequential stripping of their constituent prefix and suffix parts.  
   * Example: `keadilan` (root `adil`).  
     * Suffix stripping: `~an` is stripped. `current_word_to_process` \= `keadil`.  
     * Prefix stripping: `ke~` is stripped. `current_word_to_process` \= `adil`.  
     * Result: `ke~adil~an`.  
   * The order of rules in `aturan_afiks` and the iterative stripping logic must respect standard derivational order.

#### **3.3.5 Step 5: Handling Loanwords with Indonesian Affixes**

* If, after initial stemming (Step 3.3.3), `root_word == current_word_to_process` but the word is NOT in `kamus_dasar` (i.e., it's an OOV for the dictionary):  
  1. Attempt to strip known Indonesian prefixes and suffixes (using a simplified version of Step 3.3.4, focusing on common affixes like `di-`, `meN-`, `-kan`, `-i`).  
  2. If stripping an affix results in a remaining base that is found in an auxiliary English dictionary or a curated loanword list (or passes a heuristic like "contains only English alphabet characters and is of reasonable length"):  
     * Consider the stripped part as a valid affix and the remainder as the `loanword_base`.  
     * Example: `di-download` \-\> `di~download`. `mengkompilasi` \-\> `meN~kompilasi`.  
  * This step helps avoid treating affixed loanwords as unsegmentable units.

#### **3.3.6 Step 6: Output Formatting**

2. Assemble the final segmented string:  
   * Join `identified_prefixes` with `~`.  
   * Append the `root_word`.  
   * Append `reduplication_segments` (if any).  
   * Join and append `identified_suffixes` (in their correct order) with `~`.  
   * Ensure no leading/trailing `~` and no double `~~`.  
   * **Final Output Structure Example:** `prefix1~prefix2~ROOT~suffix1~suffix2~particle~possessive_pronoun` or `ROOT~REDUPLICATION_MARKER~affix_if_any`.  
     * `mempermainkanlah` \-\> `meN~per~main~kan~lah`  
     * `buku-bukunya` \-\> `buku~ulg~nya`  
     * `keberhasilan` \-\> `ke~ber~hasil~an` (assuming `ber-` is identified as an inner prefix to `hasil` before `ke-an` is fully resolved, or stemmer returns `hasil` for `berhasil`).

### **3.4 Ambiguity Resolution Strategies (Implementation Focus)**

* **Dictionary Check:** After each potential strip (prefix or suffix), the resulting `current_word_to_process` should be checked against `kamus_dasar`. If it's a valid root, this provides strong evidence for the strip.  
* **Rule Prioritization:**  
  * Inflectional affixes (particles, possessives) are generally stripped before derivational ones.  
  * For derivational affixes, a predefined order based on linguistic principles (e.g., Sastrawi's visitor pattern) can be adapted.  
* **Longest Match Principle:** When multiple affix rules could apply, prefer the one that matches the longest affix sequence (e.g., `per-` over `pe-` if both could lead to a valid stem, and `per-` is a valid prefix in that context).  
* **Backtracking (Limited):** If a sequence of strips leads to an invalid state (e.g., remaining `current_word_to_process` is not `root_word` and cannot be further processed), the algorithm might need to backtrack and try an alternative stripping rule if one was available at a previous step. This adds complexity and should be used judiciously.  
* **Default to Stemmer:** If complex affix interactions cannot be resolved by rules, the segmentation might default to `PREFIX_CANDIDATE ~ root_word ~ SUFFIX_CANDIDATE` where candidates are raw differences from alignment, or even just `original_word` if `root_word` is identical and no affixes are clearly identifiable.

### **3.5 Reconstruction Algorithm (for Validation and Application)**

This algorithm takes the segmented string and reconstructs the original word.

1. **Input:** Segmented string (e.g., `meN~per~main~kan~lah`).  
2. **Parsing:** Split the string by `~` into a list of morphemes. Identify the root, prefixes, suffixes, and any special markers (`~ulg~`, `~rp~`, etc.).  
3. **Core Reconstruction:**  
   * Start with the `root_word`.  
   * **Apply Suffixes:** Iterate through suffixes *from the list's end towards the root* (i.e., apply derivational suffixes like `-kan` first, then possessives, then particles). Concatenate them. Indonesian suffixation has few morphophonemic changes.  
   * **Apply Reduplication:** If a reduplication marker is present, apply the corresponding reduplication process to the (potentially suffixed) root.  
     * `buku~ulg~nya` \-\> `buku` \+ `~nya` \-\> `bukunya`. Then `bukunya~ulg` \-\> `buku-bukunya`.  
     * `main~ulg` \-\> `main-main`.  
   * **Apply Prefixes:** Iterate through prefixes *from the list's end towards the root* (i.e., apply the innermost prefix first, like `per-` before `meN-` in `meN~per~main`).  
     * For each prefix, apply its **forward morphophonemic rules** based on the `aturan_afiks` and the initial character(s) of the current word form.  
       * Example: `meN-` \+ `permainkan` (`p` initial) \-\> `mem` \+ `permainkan` \-\> `mempermainkan`.  
       * Example: `ber-` \+ `ajar` \-\> `belajar`.  
4. **Output:** Reconstructed original word.  
5. **Validation:** Compare reconstructed word with the original input word (after initial normalization) to verify correctness.

### **3.6 Implementation Guidelines and Data Structures**

* **Main Class/Module (`ModernKataKupas`):**  
  * Constructor: Initializes `kamus_dasar`, `stemmer_internal`, and loads `aturan_afiks`.  
  * Public method: `segment(word: str) -> str`  
  * Public method: `reconstruct(segmented_word: str) -> str`  
  * Private helper methods for each step of the algorithm (normalization, reduplication, alignment, prefix/suffix stripping, morphophonemic rule application).  
* **String Alignment Function:** A standalone, efficient implementation of Needleman-Wunsch.  
* **Affix Rule Engine:**  
  * Functions to lookup affix rules from the loaded JSON/YAML.  
  * Functions to apply reverse morphophonemic rules (for segmentation).  
  * Functions to apply forward morphophonemic rules (for reconstruction).  
* **Testing:** Extensive unit tests are critical for each component and for end-to-end segmentation and reconstruction, covering all morphological phenomena and edge cases discussed.

This detailed breakdown in Chapter 3 should provide a solid foundation for rewriting the codebase for "ModernKataKupas" with a clear, modern, and rule-driven approach.  